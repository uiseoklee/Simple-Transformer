{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 64    # 배치 사이즈\n",
    "M = 10    # 토큰의 최대 길이\n",
    "V = 1024    # 토큰의 개수\n",
    "N = 8    # 멀티헤드 개수\n",
    "H = 512    # 토큰의 임베딩 사이즈\n",
    "EXP = 2048    # 확장 사이즈 (FeedForward 클래스 참고)\n",
    "L = 6    # 인코더/디코더 레이어 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value):\n",
    "    scale = query.shape[-1]\n",
    "    score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(scale)\n",
    "    prob = F.softmax(score, dim=-1)\n",
    "    attn = torch.matmul(prob, value)\n",
    "    return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_head, hidden_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.dk = hidden_size // self.num_head\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        '''\n",
    "        x = torch.rand((B, M, H))\n",
    "        m = MultiHeadAttention(N, H)\n",
    "        v = m(x, x, x)\n",
    "        v.shape  # torch.Size([64, 10, 512)])\n",
    "        '''\n",
    "        n_batch = query.shape[0]\n",
    "        query = query.view(n_batch, -1, self.num_head, self.dk).transpose(1, 2)\n",
    "        key = key.view(n_batch, -1, self.num_head, self.dk).transpose(1, 2)\n",
    "        value = value.view(n_batch, -1, self.num_head, self.dk).transpose(1, 2)\n",
    "        \n",
    "        x = attention(query, key, value)\n",
    "        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.dk * self.num_head)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiHeadAttention 클래스의 내부를 보면 query, key, value의 값을 만드는데 view 함수와 transpose 함수를 사용한 것을 확인할 수 있다. 이 과정을 통해서 (B, M, H)였던 입력값이 (B, N, M, dk)로 변형됐고, 이 값과 attention 함수를 이용해서 셀프어텐션을 수행한다. 참고로 query, key, value의 값이 모두 같을 경우 어텐션은 셀프어텐션을 수행하게 되고, query, key, value의 값이 다를 경우 일반적인 어텐션을 수행하게 된다. 디코더에서의 구조도 인코더와 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size, expand_size):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(hidden_size, expand_size)\n",
    "        self.linear_2 = nn.Linear(expand_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x = torch.rand((B, M, H))\n",
    "        m = FeedForward(H, EXP)\n",
    "        v = m(x)\n",
    "        v.shape    # torch.Size([64, 10, 512])\n",
    "        '''\n",
    "        x = self.linear_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, n_vocab, hidden_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(n_vocab, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        data = np.random.randint(0, V, (B, M))\n",
    "        x = torch.from_numpy(data)\n",
    "        m = Embedding(V, H)\n",
    "        v = m(x)\n",
    "        v.shape    # torch.Size([64, 10 512])\n",
    "        '''\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드를 보면 Embedding 레이어에서 H 사이즈만큼으로 임베딩을 한 후 EncoderLayer를 몇 번 반복해서 돌리는 구조를 띠고 있다. EncoderLayer 내부를 살펴보려면 아래 EncoderLayer 클래스를 참고하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pos_encoding = torch.zeros(M, hidden_size)\n",
    "        position = torch.arange(0, M).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_size, 2) *\n",
    "                             -(math.log(10000.0) / hidden_size))\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pos_encoding = pos_encoding.unsqueeze(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x = torch.rand((B, M, H))\n",
    "        m = PositionalEncoding(H)\n",
    "        v = m(x)\n",
    "        v.shape    # torch.Size([64, 10, 512])\n",
    "        '''\n",
    "        x = x + Variable(self.pos_encoding[:, :x.size(1)],\n",
    "                         requires_grad=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트랜스포머에서는 RNN과 같은 순환 구조를 사용하지 않는다. 따라서 토큰 간의 순서 정보를 학습할 경우 약간의 추가적인 순서 정보를 넣어준다. 포지셔널 인코딩은 위와 같이 구현할 수 있다.\n",
    "포지셔널 인코딩은 Embedding 레이어를 통해 임베딩된 벡터에 대해서 실행된다. 이 포지셔널 인코딩은 인코더와 디코더의 입력 모두 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(N, hidden_size)\n",
    "        self.feedforward = FeedForward(hidden_size, EXP)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x = torch.rand((B, M, H))\n",
    "        m = FeedForward(H, EXP)\n",
    "        v = m(x)\n",
    "        v.shape    # torch.Size([64, 10, 512])\n",
    "        '''\n",
    "        x = self.self_attention(x, x, x)\n",
    "        x = self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = Embedding(V, H)\n",
    "        self.position = PositionalEncoding(H)\n",
    "        self.layers = [EncoderLayer(H) for i in range(n_layers)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        data = np.random.randint(0, V, (B, M))\n",
    "        x = torch.from_numpy(data)\n",
    "        m = Encoder(L)\n",
    "        v = m(x)\n",
    "        v.shape    # torch.Size([64, 10, 512])\n",
    "        '''\n",
    "        x = self.embedding(x)\n",
    "        x = self.position(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EncoderLayer 부분을 보면 MultiHeadAttention 부분이 있고 MultiHeadAttention 레이어에서 멀티헤드 개수만큼의 어텐션 연산이 실행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_head, hidden_size):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(n_head, hidden_size)\n",
    "        self.encdec_attention = MultiHeadAttention(n_head, hidden_size)\n",
    "        self.feedforward = FeedForward(hidden_size, 2048)\n",
    "        \n",
    "    def forward(self, x, memory):\n",
    "        '''\n",
    "        x = torch.rand((B, M, H))\n",
    "        mem = copy(x)\n",
    "        m = DecoderLayer(N, H)\n",
    "        v = m(x, mem)\n",
    "        v.shape    # torch.Size([64, 10, 512])\n",
    "        '''\n",
    "        x = self.self_attention(x, memory, memory)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(V, H)\n",
    "        self.layers = [DecoderLayer(N, H) for i in range(n_layers)]\n",
    "        \n",
    "    def forward(self, x, memory):\n",
    "        '''\n",
    "        data = np.random.randint(0, V, (B, M))\n",
    "        x = torch.from_numpy(data)\n",
    "        mem = torch.rand((B, M, H))\n",
    "        m = Decoder(L)\n",
    "        v = m(x, mem)\n",
    "        v.shape    # torch.Size([64, 10, 512])\n",
    "        '''\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 두 코드를 보면 디코더의 구조도 DecoderLayer를 몇 번 반복해서 수행하는 형태인 것을 알 수 있다. 다만 DecoderLayer를 보면 MultiHeadAttention 클래스가 두 번 정의돼 있다. 하나는 셀프어텐션을 위함이고, 다른 하나는 인코더에서의 값과 어텐션 연산을 하기 위함이다. DecoderLayer의 입력값 중에서 memory가 인코더로부터 넘어온 인코더의 출력값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(L)\n",
    "        self.decoder = Decoder(L)\n",
    "        \n",
    "    def forward(self, src, dst):\n",
    "        '''\n",
    "        data = np.random.randint(0, V, (B, M))\n",
    "        src = torch.from_numpy(data)\n",
    "        data = np.random.randint(0, V, (B, M))\n",
    "        dst = torch.from_numpy(data)\n",
    "        src.shape, dst.shape\n",
    "        \n",
    "        m = Transformer()\n",
    "        v = m(src, dst)\n",
    "        v.shape    # torch.Size([64, 10, 512])\n",
    "        '''\n",
    "        src_encoded = self.encoder(src)\n",
    "        dst_decoded = self.decoder(dst, src_encoded)\n",
    "        \n",
    "        return dst_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드 주석을 보면 트랜스포머의 입력은 (B, M) 사이즈의 번역할 문장(src)과 (B, M) 사이즈의 번역된 문장(dst)으로 구성된다는 것을 알 수 있다. 트랜스포머의 출력이 (B, M, H)가 되는데 이 사이즈의 벡터는 로그 소프트맥스 함수 등을 통해서 각 (B, M) 사이즈로 변하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        data = np.random.randint(0, V, (B, M))\n",
    "        src = torch.from_numpy(data)\n",
    "        data = np.random.randint(0, V, (B, M))\n",
    "        dst = torch.from_numpy(data)\n",
    "        src.shape, dst.shape\n",
    "        \n",
    "        m = Transformer()\n",
    "        v = m(src, dst)\n",
    "        v.shape    # torch.Size([64, 10, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
